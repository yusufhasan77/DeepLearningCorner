{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST with convolutional neural network\n",
    "\n",
    "In this notebook we will build a Convolutional neural networks (CNNs) and train it on the Fashion-MNIST dataset. CNNs are widely used  in computer vision. We will be discussing CNNs in more detail in the Neural Network Architecture section of this notebook.<br>\n",
    "In this notebook we will be using the keras api to build our cnn. With keras a model can be quickly built.<br>\n",
    "Before we build the neural network lets get a closer look at the dataset we will be working with as well import the packages which will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using MXNet backend\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  #The numpy package provides a collection of basic routines for manipulating large arrays and matrices\n",
    "import idx2numpy    #This package will convert our data from idx3-ubyte format to a numpy array\n",
    "import PIL          #This package provides image processing capabilities, it will be used to visualize our data\n",
    "import time         #The time package provides functions to find out the running time of our algorithms\n",
    "\n",
    "from PIL import Image #Used to save our Image\n",
    "from matplotlib import pyplot as plt #This function will be used to inconjunction with PIL package to visualize our data\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Dropout\n",
    "from keras.layers import MaxPooling2D, Dropout\n",
    "from keras.models import Model, load_model \n",
    "from keras.utils  import to_channels_first\n",
    "\n",
    "from sklearn.metrics import classification_report #To generate the classification report containing precison, recall, f1-score,etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will be using the Fashion-MNIST dataset. The data was collected by Zolando Research as a replacement for MNIST.[[1]](#1)\n",
    "<br>\n",
    "This dataset has pictures of every Zolando's fashion product. There are 60000 images in the \"train-images-idx3-ubyte\" with the corresponding labels in \"train-labels-idx1-ubyte\". Each image is a 28x28 8-bit greyscale picture.\n",
    "\n",
    "We will concatenate the train and test data together, randomly shuffle it and divide it into three sets. The Training set, the dev/cross-validation set and the test set. The training set will consist of 60000 images, the dev set will consist of 5000 images and the test set will have 5000 images.\n",
    "The reason for doing this is that we can use the dev set to tune our hyperparameters. The test set will be used to give us an unbiased accuracy of our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the training data and labels\n",
    "X_train = idx2numpy.convert_from_file('data/train-images-idx3-ubyte')\n",
    "Y_train = idx2numpy.convert_from_file('data/train-labels-idx1-ubyte')\n",
    "\n",
    "#load the test data and labels\n",
    "X_test = idx2numpy.convert_from_file('data/t10k-images-idx3-ubyte')\n",
    "Y_test = idx2numpy.convert_from_file('data/t10k-labels-idx1-ubyte')\n",
    "\n",
    "#Before we proceed further we will take the first image from the unshuffled X_train and save it to a file.\n",
    "#We will use this image in the next notebook for visualizing the feature(activation) maps of our network.\n",
    "img = X_test[0,:,:] #Retrieve the first image from the array and save it to a variable\n",
    "img = Image.fromarray(img) #Convert it to image using PIL library's image function\n",
    "\n",
    "img.save(\"Image_Visualization.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the data together\n",
    "X_shuffled = np.concatenate((X_train,X_test), axis=0)\n",
    "Y_shuffled = np.concatenate((Y_train,Y_test), axis=0)\n",
    "\n",
    "#Randomly shuffle the data\n",
    "permutation = list(np.random.permutation(X_shuffled.shape[0]))\n",
    "X_shuffled = X_shuffled[permutation,:,:]\n",
    "Y_shuffled = Y_shuffled[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train/dev/test\n",
    "#Train set\n",
    "X_train = X_shuffled[0:60000,:,:] #Train Data\n",
    "Y_train = Y_shuffled[0:60000] #Corresponding Labels\n",
    "\n",
    "X_dev = X_shuffled[60000:65000,:,:] #Dev/cross-validation Data\n",
    "Y_dev = Y_shuffled[60000:65000] #Corresponding Labels\n",
    "\n",
    "X_test = X_shuffled[65000:70000,:,:] #Test Data\n",
    "Y_test = Y_shuffled[65000:] #Corresponding Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<u>Table 1</u> below gives a summary of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Pictures/Fashion-MNIST_visual_table.JPG)\n",
    "\n",
    "<caption><center> <u>Table_1</u>: Name of classes and example images in our data set. <br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will create a list with these label names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names= [\"T=Shirt/Top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandals\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boots\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "With the code below we can inspect the individual images in the training dataset. Note: To view images in other datasets(dev/test) replace X_train and Y_train in the code below with the array for those datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image belongs to class: 8\n",
      "It is a: Bag\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAATgUlEQVR4nO3dXYyc1X0G8OeZ2ZlZ73rX6/XHshgn5sOI0FSx6cbQ0kZUtIhwEZOLRPFFRFQk5yJIQYqqovQiVMoFqppEvaiiOoXGrVIipATBBUpjWZEQSkVYkDF2nGDjGNt4vesv7P2er38vdmg3Zs//LPPdnucnrWZ3zr4zZ9+ZZ97Z+b/nHJoZROT/v0ynOyAi7aGwiyRCYRdJhMIukgiFXSQRPe28szwL1ov+dt6lSFIWMIuiLXKltobCTvJBAP8IIAvgX8zsKe/3e9GPu3l/I3cpIo5X7WCwre638SSzAP4JwGcB3AlgD8k76709EWmtRv5n3wXghJmdNLMigB8D2N2cbolIszUS9i0Aziz7+Wztut9Dci/JcZLjJSw2cHci0ohGwr7ShwAfOvfWzPaZ2ZiZjeVQaODuRKQRjYT9LICty36+CcC5xrojIq3SSNhfA7Cd5M0k8wC+BODF5nRLRJqt7tKbmZVJPgbgP7FUenvGzI42rWci0lQN1dnN7CUALzWpLyLSQjpdViQRCrtIIhR2kUQo7CKJUNhFEqGwiyRCYRdJhMIukgiFXSQRCrtIIhR2kUQo7CKJUNhFEqGwiyRCYRdJhMIukgiFXSQRCrtIIhR2kUQo7CKJUNhFEqGwiyRCYRdJhMIukgiFXSQRCrtIIhR2kUQo7CKJUNhFEtHQKq7N9va//pHbvuadQrBt67d/6W57bc89bnulQLc9P10NtlV7/G17r5Td9kwxfNsAkJ0rue0shbenmbut0e+7FbINbV8azAXbqjn/WFO4UnTbEfvbMuG+WeS+y33+3z1zgx+duRsj+9W5+c/t9p/Lbzy+M9w4Ht62obCTPAVgGkAFQNnMxhq5PRFpnWYc2f/czC424XZEpIX0P7tIIhoNuwH4OcnXSe5d6RdI7iU5TnK8hMUG705E6tXo2/h7zewcyc0ADpD8jZm9vPwXzGwfgH0AMMhh/xMVEWmZho7sZnaudjkF4HkAu5rRKRFpvrrDTrKf5MAH3wN4AMCRZnVMRJqrkbfxIwCe51KdtQfAf5jZz7wNmMuhZ+TGYPvf/fEL7h3+8+hngm3FBz/tbtv7VxNu+13DZ9z2381uCLb19fh18DVZv/3SYp/bXqz6D1O5Gn7NLpv/ep6lX+MvZCt13zcA9GXC2w/m5yO37de6Y/u1kA2f37Am49fwvccbALYX5tz2Cwtr3fbpUvickcc3vOJu+8UNdwfbqj3hx6PusJvZSQCfqnd7EWkvld5EEqGwiyRCYRdJhMIukgiFXSQR7R3iSgCZ8OvLxfKgu/mnN50OtlW/7ZfOHho67LYfuPoHbvsdA5PBts35a+62fZEyT9X84ZB9Gf804yzDJyZOV3rdbYd7Ztz2/sh9l6z+p1AGftlvurrGbY/1Lcdw6a0SKUnuXu+X1uaq4dIZEH/MzpTCpb2TZb8Uu+b8QrAt4wx31pFdJBEKu0giFHaRRCjsIolQ2EUSobCLJEJhF0lEW+vs1d4c5j9xQ7C9kHnT3f707Ppg29hQuAYPAKeKG932M/Ph2waAjYXZYFtf1q+jH53Z4rbPV8LTLQPAHWvPu+2FTHio59VIzbYSeb0vZf1682yk3lzy5kyOqEZq4ZVsZPpvhofXLpi/z98tbnLbY9Zlw88XAOh1HrNbevx9Pj8SPnfCm55bR3aRRCjsIolQ2EUSobCLJEJhF0mEwi6SCIVdJBFtX7LZG7o9kPGnFr44H56e96bNl9xtB7PhMcAAMFfOu+23Df0u2PbbuRF32x0D/jkAx2bD02sDwK6+d9z248XwuQuljP8Qx+rgsTr61Yo/5rzXGVM+WfLnLxjJ+fMEeLVqAFiohmvpsbH0N+auuO2XKpGpoiNj8b25G7JrzrnbsuotrBRu05FdJBEKu0giFHaRRCjsIolQ2EUSobCLJEJhF0lE2+eNN2cMcmwu7mIlXBPe3DPtbhurF/dGlv/16tFDOf/8gNOL/vK/fzF01G0/VfLHVm/LXwi25fL+kssl+HX2hap//sGO3nfd9vcr4fH0W/P+uRHevO9AfM76C1W/ju8Ziozjn4qcI+DNMQD4Y/XPlP2x9pbxxvGH26JHdpLPkJwieWTZdcMkD5A8Xrv0Z34QkY5bzdv4HwJ48LrrngBw0My2AzhY+1lEulg07Gb2MoDL1129G8D+2vf7ATzc5H6JSJPV+wHdiJlNAEDtcnPoF0nuJTlOcrxU9OflEpHWafmn8Wa2z8zGzGwsl+9v9d2JSEC9YZ8kOQoAtcup5nVJRFqh3rC/COCR2vePAHihOd0RkVaJ1tlJPgvgPgAbSZ4F8C0ATwF4juSjAE4D+MJq7swyRLkv/PoyEBlzXqp4c2L745Nj47YXInO3e2OjY6bL/hrpJxeDH3kAAHLO/OcAkHXGMMfGVQ/F5jePnH9wvrzObffmbj9f8rf9s74TbnuO/mPex/Aa6d4cAEC8hr828lzdXvDn+j+5GJ4D4VKlNf/uRsNuZnsCTfc3uS8i0kI6XVYkEQq7SCIUdpFEKOwiiVDYRRLR3iGuBnhVpF76ZZ58T3jjXy/6yyLfUZhw20fX+NMWe8pV/zUzn/GHap6Y90tv9w4ed9tfmbndbW/Ebb2TbvuxeX8a7L5MeDnrWDn0Uq9fNpyu+iXNS+XwdM9bIlNFx0q5cxV/yHRsSPWCU9rLRkqK5d7wMFZvlWsd2UUSobCLJEJhF0mEwi6SCIVdJBEKu0giFHaRRLR/Kmnn5SVWX1xXCA8rHL+6zd32Yt+A2x7jDTPd1utPiXy26E++u77Hn7Y4tpT1jFPzXZsND/ME/Do4ABQjQz1H8++77YvO0OCc+UN335jf5rZvikwfvqFnJtg2HVlqui/j77dh57YB4LJT4wf8YcmbIsOOK3lnKmmnSUd2kUQo7CKJUNhFEqGwiyRCYRdJhMIukgiFXSQR7R/PHi4vRqdUrjhF+tHeq+62sWmq1/X7te5SNbyrDk1vdbddE5mOeeMav158ruTX6QvOePn3S+ElkwHgk4Nn3fZrkamoY2PSb8pfv0zg/zqxEJ5OGYgvmxzjPV9if1dvZMnlWN9i23vj3b0aPAC4u1x1dhFR2EUSobCLJEJhF0mEwi6SCIVdJBEKu0gi2lpnpxkyxXANcWOPP3d7f0947PW1yLLIWwr+POHvLfq17FlnzPjt/f7c6u8ubHDbzywMu+13rX3Xbf9YITye/mrWr7NH6830x7tfMX95YW9cd6xGHxtLP5z1x5SfKYX3e2zeeG+5ZwD41dwtbvuOfv8xm674z1ePt9u8Cn30yE7yGZJTJI8su+5Jku+RPFT7eugj9VZE2m41b+N/CODBFa7/npntqH291NxuiUizRcNuZi8DCJ/zKCL/JzTyAd1jJA/X3uYH/+EluZfkOMnxUtGfW0tEWqfesH8fwK0AdgCYAPCd0C+a2T4zGzOzsVze/zBHRFqnrrCb2aSZVcysCuAHAHY1t1si0mx1hZ3k6LIfPw/gSOh3RaQ7ROvsJJ8FcB+AjSTPAvgWgPtI7sBSWe8UgK+u5s4sS5TWhl9fbujxx6RnnHnlN+T8zwOOzY667fOV8PzmADDaGz4HIFYvHujxx9IP9zT2WUbVGbd9NTI/+k15f877/si88iM5/zHzeI8nAMxV8267V0cH/Pn2F5z57AGgQv84ePua8277YMZ/zCdtKNi2KevvF3PGrHuiYTezPStc/XR9dycinaLTZUUSobCLJEJhF0mEwi6SCIVdJBFtHeJq8JdsPl9e527vlZgWnamegXhpbSjnL4vsLU0cWy56e/+U2+4tBw0AkyV/v6x3Snc7+/yhlu9X/CGwE5FprL37BoA8w9Ncx0qWsSWZ+2PLKjtDYC9X/CWVc06/gfjy4pcauP2i+VNJe5Vcb6p2HdlFEqGwiyRCYRdJhMIukgiFXSQRCrtIIhR2kUS0t86eAUp94fF5g5FlldfmwnXVycVBd9uRgj9N9XxkOOWcM5V07LZjYkM9K5HhmItO+/nI33WxNBC5bf8p4i2LDPj16NHIdM5bc/7w29g5AnPOssgZ+Pv8VHGT2x47NyJ2DoDXt4uRc0KKA+EMeQ+HjuwiiVDYRRKhsIskQmEXSYTCLpIIhV0kEQq7SCLaW2fvARaH65wHF0DZKSJ+vM9fji5WF61EXve8KZnPzPtjvjcW/DHfO/tOue0ni5vddq9OP132a9ExhYw/rjs2Jv2Kc/+x8wsulv1zJyrwn0vrsuE5CqZK/m3HxunH/u5eltx2zzUL1+ABwDnlQ3V2EVHYRZKhsIskQmEXSYTCLpIIhV0kEQq7SCLaW2fPAovD4Ymtjy+OuNtnEN52tuzXJqcW/Xm81+X8sfTentpcCM9PDgCFjF9zPTz/Mbc9NmZ8wJkHILbs8dszsRq+P4f5Dc5S1gCQdcaNe+sAAMBipJbtPR8AYJHhceFzFX+/3Fy44LZfLvvPp9gaCF6dPg//nBAvQ94uix7ZSW4l+QuSx0geJfn12vXDJA+QPF679M8sEZGOWs3b+DKAb5jZJwDcA+BrJO8E8ASAg2a2HcDB2s8i0qWiYTezCTN7o/b9NIBjALYA2A1gf+3X9gN4uFWdFJHGfaQP6EhuA7ATwKsARsxsAlh6QQCw4j9/JPeSHCc5Xpn1zzcWkdZZddhJrgXwEwCPm9mqZ1g0s31mNmZmY9n+/nr6KCJNsKqwk8xhKeg/MrOf1q6eJDlaax8F4C9VKiIdFS29kSSApwEcM7PvLmt6EcAjAJ6qXb4Qva0KkLsaHpa4vTDpbv+b7GiwbSg35277wNBbbvsf5v3XqgvO1L+x4Y53Rsp667P+MNTnZvwyzgZnaeJZp98A8Ncbf+W2X67Epkz2h5l6R5NsZIjq5ao/BHbW/KfvDdlw32NHuV76j+mc+fvlwJxfTvXMml8WXHc83HbOmcF6NXX2ewF8GcBbJA/VrvsmlkL+HMlHAZwG8IVV3JaIdEg07Gb2ChB8Cb6/ud0RkVbR6bIiiVDYRRKhsIskQmEXSYTCLpIImvnDBJtpkMN2N8Mf4P/Jm0V3+/+6dHOw7fh7/lBNu+bXLq3Hr+nmroQLF7lrkemxI7s4UqZHLnKW8cJG5w78Pyv6ct8z4/9tkTI+yr3hvlX6Ip2L7BeLDL9Fr3P7xcgfHukazd8v1udPwb3z1tPBtvX58BTYAHD2nvB5Fa/aQVyzyyt2Tkd2kUQo7CKJUNhFEqGwiyRCYRdJhMIukgiFXSQRbZ1KOuaXn/Jr4cB7wZbbnDaRbuOdOtGqydt0ZBdJhMIukgiFXSQRCrtIIhR2kUQo7CKJUNhFEqGwiyRCYRdJhMIukgiFXSQRCrtIIhR2kUQo7CKJUNhFEhENO8mtJH9B8hjJoyS/Xrv+SZLvkTxU+3qo9d0VkXqtZvKKMoBvmNkbJAcAvE7yQK3te2b2D63rnog0y2rWZ58AMFH7fprkMQBbWt0xEWmuj/Q/O8ltAHYCeLV21WMkD5N8huT6wDZ7SY6THC9hsaHOikj9Vh12kmsB/ATA42Z2DcD3AdwKYAeWjvzfWWk7M9tnZmNmNpZDZGEwEWmZVYWdZA5LQf+Rmf0UAMxs0swqZlYF8AMAu1rXTRFp1Go+jSeApwEcM7PvLrt+dNmvfR7AkeZ3T0SaZTWfxt8L4MsA3iJ5qHbdNwHsIbkDSwsSnwLw1Zb0UESaYjWfxr8CYKX1nl9qfndEpFV0Bp1IIhR2kUQo7CKJUNhFEqGwiyRCYRdJhMIukgiFXSQRCrtIIhR2kUQo7CKJUNhFEqGwiyRCYRdJBM2sfXdGXgDw7rKrNgK42LYOfDTd2rdu7RegvtWrmX37uJltWqmhrWH/0J2T42Y21rEOOLq1b93aL0B9q1e7+qa38SKJUNhFEtHpsO/r8P17urVv3dovQH2rV1v61tH/2UWkfTp9ZBeRNlHYRRLRkbCTfJDkb0meIPlEJ/oQQvIUybdqy1CPd7gvz5CcInlk2XXDJA+QPF67XHGNvQ71rSuW8XaWGe/ovuv08udt/5+dZBbA2wD+EsBZAK8B2GNmv25rRwJIngIwZmYdPwGD5GcAzAD4NzP7ZO26vwdw2cyeqr1Qrjezv+mSvj0JYKbTy3jXVisaXb7MOICHAXwFHdx3Tr++iDbst04c2XcBOGFmJ82sCODHAHZ3oB9dz8xeBnD5uqt3A9hf+34/lp4sbRfoW1cwswkze6P2/TSAD5YZ7+i+c/rVFp0I+xYAZ5b9fBbdtd67Afg5yddJ7u10Z1YwYmYTwNKTB8DmDvfnetFlvNvpumXGu2bf1bP8eaM6EfaVlpLqpvrfvWZ2F4DPAvha7e2qrM6qlvFulxWWGe8K9S5/3qhOhP0sgK3Lfr4JwLkO9GNFZnaudjkF4Hl031LUkx+soFu7nOpwf/5HNy3jvdIy4+iCfdfJ5c87EfbXAGwneTPJPIAvAXixA/34EJL9tQ9OQLIfwAPovqWoXwTwSO37RwC80MG+/J5uWcY7tMw4OrzvOr78uZm1/QvAQ1j6RP4dAH/biT4E+nULgDdrX0c73TcAz2LpbV0JS++IHgWwAcBBAMdrl8Nd1Ld/B/AWgMNYCtZoh/r2p1j61/AwgEO1r4c6ve+cfrVlv+l0WZFE6Aw6kUQo7CKJUNhFEqGwiyRCYRdJhMIukgiFXSQR/w2uMBzZIouSiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0 #Change this to get a different image, python indexes from 0 therefore the index for our dataset will be from 0-59999\n",
    "\n",
    "image = X_train[index] #The image we want\n",
    "plt.imshow(image) #Display the image\n",
    "\n",
    "print(\"Image belongs to class: \"+str((Y_train[index])))\n",
    "print(\"It is a: \"+label_names[Y_train[index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "First Lets convert our input feature matrices <b>X_train</b>, <b>X_dev</b> and <b>X_test</b> to (number of examples,28,28,1) as this is the shape of the input that keras expects. Lets also have a look at the shape of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)\n",
    "X_dev = X_dev.reshape(X_dev.shape[0],X_dev.shape[1],X_dev.shape[2],1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of training data is: (60000, 28, 28, 1)\n",
      "The Shape of training labels is: (60000,)\n",
      "The Shape of dev data is: (5000, 28, 28, 1)\n",
      "The Shape of dev labels is: (5000,)\n",
      "The Shape of test data is: (5000, 28, 28, 1)\n",
      "The Shape of test labels is: (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The Shape of training data is:\",X_train.shape)\n",
    "print(\"The Shape of training labels is:\",Y_train.shape)\n",
    "\n",
    "print(\"The Shape of dev data is:\",X_dev.shape)\n",
    "print(\"The Shape of dev labels is:\",Y_dev.shape)\n",
    "\n",
    "print(\"The Shape of test data is:\",X_test.shape)\n",
    "print(\"The Shape of test labels is:\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br><b>Y_train</b>,<b>Y_dev</b> and <b>Y_test</b> are (60000,), (5000,) and (5000,) arrays. For our neural network we will convert them to (60000,10) arrays (one hot encoding).</br>\n",
    "The code below will carry out this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 10               #Number of classes for our dataset\n",
    "Y_train = np.eye(classes)[Y_train.reshape(-1)] #One hot encode Y_train\n",
    "Y_dev = np.eye(classes)[Y_dev.reshape(-1)] #One hot encode Y_dev\n",
    "Y_test = np.eye(classes)[Y_test.reshape(-1)] #One hot encode Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(5000, 10)\n",
      "(5000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)\n",
    "print(Y_dev.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output matches our expectation. Lets move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Standardizing the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will standardize our input feature matrices <b>X_train</b>, <b>X_dev</b> and <b>X_test</b>. This will help our algorithm converge faster. Generally we standardize the data by subtracting the mean of input matrix from each value in it and dividing by the standard deviation of of the matrix, but for image data dividing each value of the input matrix with 255(max value of a pixel) works as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255 #Normalize the matrix\n",
    "X_dev = X_dev/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is processed and we are ready to feed it to our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision to use convolutional neural networks is based on the following:- <br>\n",
    "A MLP has no built-in invariance with respect to translations and local distortions of the input. Our neural network should classify a shirt as a shirt if it is translated by some units. <br>\n",
    "The sparsity of connections in CNN lowers the numbers of trainable parameters therefore making it computational feasible to train the network compared to a MLP for which the the number of trainable parameters will be very high if the image resolution is high. The large number of parameters also means that we would need a large amount of data if the network is not to overfit.[[2]](#b)<br>\n",
    "\n",
    "The CNN model used in this notebook is inspired from VGG-16.[[3]](#c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_conv(input_shape):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    #Convolution->BatchNorm->ReLUx3\n",
    "    X = Conv2D(8, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', bias_initializer='zeros', name='Block1_conv1a')(X_input)\n",
    "    X = BatchNormalization(axis=1, name='Block1_bn1a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(8, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', bias_initializer='zeros', name='Block1_conv1b')(X)\n",
    "    X = BatchNormalization(axis=1, name='Block1_bn1b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(8, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', bias_initializer='zeros', name='Block1_conv1c')(X)\n",
    "    X = BatchNormalization(axis=1, name='Block_bn1c')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    #MaxPool\n",
    "    X = MaxPooling2D((2,2), name='Block1_maxpool')(X)\n",
    "    X = Dropout(0.1)(X)\n",
    "    \n",
    "    #Convolution->BatchNorm->ReLUx3\n",
    "    X = Conv2D(16, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', bias_initializer='ones', name='Block2_conv2a')(X)\n",
    "    X = BatchNormalization(axis=1, name='Block2_bn2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(16, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', bias_initializer='ones', name='Block2_conv2b')(X)\n",
    "    X = BatchNormalization(axis=1, name='Block2_bn2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(16, (3,3), strides=(1,1), padding='same', kernel_initializer='he_normal', bias_initializer='ones', name='Block2_conv2c')(X)\n",
    "    X = BatchNormalization(axis=1, name='Block2_bn2c')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    #MaxPool\n",
    "    X = MaxPooling2D((2,2), name='Block2_maxpool')(X)\n",
    "\n",
    "    #Flatten X(convert it to a vector) and feed it to fully connected layers\n",
    "    X = Flatten()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(128, activation='relu', kernel_initializer='he_normal', bias_initializer='zeros', name='fc1')(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense(10, activation='softmax', name='softmax')(X)\n",
    "\n",
    "    # Create model. This creates our Keras model instance, we'll use this instance to train/test the model.\n",
    "    model = Model(inputs=X_input, outputs=X, name='model_conv')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backend used for Keras is Mxnet. Mxnet expects the input to be channels first. We will convert this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train = to_channels_first(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_conv(X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the keras' default learning rate for ADAM. However this is probably the most important hyperparameter. If we want to tune the model this should be the first thing we should tweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will print the summary of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1, 28, 28)         0         \n",
      "_________________________________________________________________\n",
      "Block1_conv1a (Conv2D)       (None, 8, 28, 28)         80        \n",
      "_________________________________________________________________\n",
      "Block1_bn1a (BatchNormalizat (None, 8, 28, 28)         32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8, 28, 28)         0         \n",
      "_________________________________________________________________\n",
      "Block1_conv1b (Conv2D)       (None, 8, 28, 28)         584       \n",
      "_________________________________________________________________\n",
      "Block1_bn1b (BatchNormalizat (None, 8, 28, 28)         32        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8, 28, 28)         0         \n",
      "_________________________________________________________________\n",
      "Block1_conv1c (Conv2D)       (None, 8, 28, 28)         584       \n",
      "_________________________________________________________________\n",
      "Block_bn1c (BatchNormalizati (None, 8, 28, 28)         32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 28, 28)         0         \n",
      "_________________________________________________________________\n",
      "Block1_maxpool (MaxPooling2D (None, 8, 14, 14)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 14, 14)         0         \n",
      "_________________________________________________________________\n",
      "Block2_conv2a (Conv2D)       (None, 16, 14, 14)        1168      \n",
      "_________________________________________________________________\n",
      "Block2_bn2a (BatchNormalizat (None, 16, 14, 14)        64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 14, 14)        0         \n",
      "_________________________________________________________________\n",
      "Block2_conv2b (Conv2D)       (None, 16, 14, 14)        2320      \n",
      "_________________________________________________________________\n",
      "Block2_bn2b (BatchNormalizat (None, 16, 14, 14)        64        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 14, 14)        0         \n",
      "_________________________________________________________________\n",
      "Block2_conv2c (Conv2D)       (None, 16, 14, 14)        2320      \n",
      "_________________________________________________________________\n",
      "Block2_bn2c (BatchNormalizat (None, 16, 14, 14)        64        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 14, 14)        0         \n",
      "_________________________________________________________________\n",
      "Block2_maxpool (MaxPooling2D (None, 16, 7, 7)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 109,114\n",
      "Trainable params: 108,970\n",
      "Non-trainable params: 144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() #Shows the summary of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the summary above the fully connected layer has the highest number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Time to train the model the cell below will start training the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   64/60000 [..............................] - ETA: 2:55 - loss: 4.7469 - acc: 0.0469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\mxnet\\module\\bucketing_module.py:411: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.015625). Is this intended?\n",
      "  force_init=force_init)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.7131 - acc: 0.7392\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.4443 - acc: 0.8338\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.3872 - acc: 0.8567\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.3535 - acc: 0.8691\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.3267 - acc: 0.8815\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.3127 - acc: 0.8852\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.3019 - acc: 0.8904\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.2885 - acc: 0.8943\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.2826 - acc: 0.8954\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.2740 - acc: 0.9000\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.2666 - acc: 0.9003\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.2623 - acc: 0.9036\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.2601 - acc: 0.9040\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.2543 - acc: 0.9063 4s - loss:\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 198s 3ms/step - loss: 0.2489 - acc: 0.9078\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.2477 - acc: 0.9082\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.2442 - acc: 0.9101\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 196s 3ms/step - loss: 0.2398 - acc: 0.9109\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.2382 - acc: 0.9116\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 197s 3ms/step - loss: 0.2344 - acc: 0.9138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1afdcd156a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=Y_train, epochs= 20, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training the model again to make predictions in the future, we are going to save our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Fashion_MNIST_Conv.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Let's see the classification report for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90      5980\n",
      "           1       0.99      1.00      0.99      5967\n",
      "           2       0.89      0.92      0.91      5756\n",
      "           3       0.96      0.92      0.94      6278\n",
      "           4       0.92      0.87      0.89      6332\n",
      "           5       0.99      0.99      0.99      5961\n",
      "           6       0.80      0.84      0.82      5713\n",
      "           7       0.99      0.96      0.98      6154\n",
      "           8       0.99      0.99      0.99      5977\n",
      "           9       0.97      0.99      0.98      5882\n",
      "\n",
      "    accuracy                           0.94     60000\n",
      "   macro avg       0.94      0.94      0.94     60000\n",
      "weighted avg       0.94      0.94      0.94     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_train = model.predict(X_train) #Generate predictions for the training data\n",
    "\n",
    "predictions_train = np.argmax(predictions_train, axis = 1) #Index of the max value in the rows of prediction matrix\n",
    "labels_train = np.argmax(Y_train, axis = 1) #Index of the max value in the rows of label matrix\n",
    "\n",
    "print(classification_report(predictions_train,labels_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>The cells below evaluates the model on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = to_channels_first(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88       454\n",
      "           1       0.99      0.99      0.99       516\n",
      "           2       0.88      0.91      0.89       486\n",
      "           3       0.95      0.90      0.93       549\n",
      "           4       0.92      0.83      0.87       551\n",
      "           5       0.99      0.99      0.99       490\n",
      "           6       0.74      0.82      0.78       471\n",
      "           7       0.99      0.93      0.96       513\n",
      "           8       0.99      0.99      0.99       496\n",
      "           9       0.94      0.99      0.96       474\n",
      "\n",
      "    accuracy                           0.92      5000\n",
      "   macro avg       0.92      0.92      0.92      5000\n",
      "weighted avg       0.93      0.92      0.92      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_dev = model.predict(X_dev)\n",
    "\n",
    "predictions_dev = np.argmax(predictions_dev, axis = 1)\n",
    "labels_dev = np.argmax(Y_dev, axis = 1)\n",
    "\n",
    "print(classification_report(predictions_dev,labels_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev-Set accuracy is  approximately same as training accuracy. The model is slightly overfitting. If we tune the hyperparameters e.g learning rate, number of layers, number of hidden units in fully connected layers and number of channels and filter sizes in convolutional layers, minibatch size, learning rate decay, etc. we might get even better accuracy<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does not perform as well on class 6. Further analysis is required as to why this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>We should now see how our model performs on the test set.The cell below will generate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = to_channels_first(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88       468\n",
      "           1       0.99      1.00      0.99       487\n",
      "           2       0.86      0.91      0.89       470\n",
      "           3       0.96      0.91      0.93       520\n",
      "           4       0.93      0.85      0.89       547\n",
      "           5       0.98      0.99      0.99       501\n",
      "           6       0.76      0.82      0.79       470\n",
      "           7       0.98      0.96      0.97       531\n",
      "           8       0.99      0.98      0.98       513\n",
      "           9       0.97      0.98      0.97       493\n",
      "\n",
      "    accuracy                           0.93      5000\n",
      "   macro avg       0.93      0.93      0.93      5000\n",
      "weighted avg       0.93      0.93      0.93      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_test = model.predict(X_test) #Generate predictions for the training data\n",
    "\n",
    "predictions_test = np.argmax(predictions_test, axis = 1) #Index of the max value in the rows of prediction matrix\n",
    "labels_test = np.argmax(Y_test, axis = 1) #Index of the max value in the rows of label matrix\n",
    "\n",
    "print(classification_report(predictions_test,labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs decently. Test-set results are pretty close to the dev-set results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> The code below will display an image from the test set, it's true class as well as the class that our algorithm has predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True class of image is: 0\n",
      "Which is a: T=Shirt/Top\n",
      "\n",
      "The NN predicts that this image belongs to class: 0\n",
      "Which is a: T=Shirt/Top\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = X_test.reshape(5000,28,28)\n",
    "\n",
    "index = 0\n",
    "image = X_test[index,:,:]\n",
    "plt.imshow(image)\n",
    "print(\"True class of image is: \"+str(np.argmax(Y_test[index])))\n",
    "print(\"Which is a: \"+label_names[np.argmax(Y_test[index])])\n",
    "\n",
    "print(\"\\nThe NN predicts that this image belongs to class: \"+str(predictions_test[index]))\n",
    "print(\"Which is a: \"+label_names[(predictions_test[index])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network we trained provides decent results however it can be improved if we tune the hyperparamters. Secondly if we have a measure of how accurately humans predict the images in this dataset we can use it as a proxy for Bayes-error. This will give us an idea of how much the model can be improved.<br>\n",
    "Our Convolutional neural network provides better results than the MLP we trained in the previous notebook.<br>\n",
    "In the next notebook we will visualize the filters and feature maps of the same network we trained here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>1. H. Xiao, K. Rasul, R. Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. *arXiv preprint arXiv:1708.07747 [cs.LG]*, 2017.\n",
    "<br>\n",
    "<a id=\"b\"></a>2. Y. Lecun, L. Bottou, Y. Bengio, P. Haffner, \"Gradient-based learning applied to document recognition\", Proc. IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.\n",
    "<br>\n",
    "<a id=\"c\"></a>3. K. Simonyan, A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. *arXiv preprint arXiv:1409.1556v6 [cs.CV]*, 2015.\n",
    "<br>\n",
    "4. Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.  Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097â€“1105, 2012.\n",
    "<br>\n",
    "5. K. He, X. Zhang, S. Ren, J. Sun. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. *arXiv preprint arXiv:1502.01852 [cs.CV]*, 2015.\n",
    "<br>\n",
    "6. Srivastava, Nitish & Hinton, Geoffrey & Krizhevsky, Alex & Sutskever, Ilya & Salakhutdinov, Ruslan. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research. 15. 1929-1958.\n",
    "<br>\n",
    "7. D. P. Kingma, J. Ba. Adam: A Method for Stochastic Optimization. *arXiv preprint arXiv:1412.6980v9 [cs.LG]*, 2017.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
